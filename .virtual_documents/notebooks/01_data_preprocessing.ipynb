import numpy as np
import pandas as pd
from pathlib import Path

pd.set_option('display.max_columns', None)


# Wczytanie wszystkich plików CSV do jednego DataFrame
folder_path = Path('../data/raw')
all_files = folder_path.glob('*.csv')

df_all = pd.concat(
    [
        pd.read_csv(f, delimiter=',', decimal=',', low_memory=False)
        for f in all_files
    ],
    ignore_index=True
)


# Czyszczenie: break-in-time-series i brakujące wartości
if 'OBS_FLAG' in df_all.columns:
    df_all = df_all[df_all['OBS_FLAG'] != 'b']
df_all = df_all.dropna(subset=['OBS_VALUE'])


# Wybór najnowszego roku dla każdego miasta i wskaźnika
df_all = df_all.sort_values(
    ['Geopolitical entity (declaring)', 'Urban audit indicator', 'TIME_PERIOD'],
    ascending=[True, True, False]
)
df_latest = df_all.drop_duplicates(
    subset=['Geopolitical entity (declaring)', 'Urban audit indicator'],
    keep='first'
)


# Pivot do szerokiego formatu (1 wiersz = 1 miasto, kolumny = wskaźniki)
df_wide = df_latest.pivot(
    index='Geopolitical entity (declaring)',
    columns='Urban audit indicator',
    values='OBS_VALUE'
).reset_index()


# Czytelne nazwy kolumn
rename_map = {
    'Population on the 1st of January, total': 'Population',
    'Age dependency ratio (population aged 0-19 and 65 and more to population aged 20-64)': 'Age_dependency_ratio',
    'Young-age dependency ratio (population aged 0-19 to population 20-64 years)': 'Young_dependency_ratio',
    'Old age dependency ratio (population 65 and over to population 20 to 64 years)': 'Old_dependency_ratio',
    'Median population age': 'Median_age',
    'Foreigners as a proportion of population': 'Share_foreigners',
    'Infant mortality rate (per 1000 live births)': 'Infant_mortality_rate',
    'Crude birth rate (per 1000 inhabitants)': 'Crude_birth_rate',
    'Number of deaths per year under 65 due to diseases of the circulatory or respiratory systems':
        'Deaths_under_65_circulatory_respiratory',
    'Crude death rate (per 1000 inhabitants)': 'Crude_death_rate',
    'Share of persons at risk of poverty or social exclusion -%': 'Share_poverty_risk',
    'Average area of living accommodation - mÂ˛/person': 'Avg_living_area_m2_per_person',
    'Average annual rent for housing per mÂ˛ - EUR': 'Avg_annual_rent_per_m2',
    'Average price for buying an apartment  per m2 - EUR': 'Avg_apartment_price_per_m2',
    'Share of students in higher education in the total population (per 1000 persons)': 'Share_students_higher_edu',
    'Share of early leavers from education and training, total -%': 'Share_early_leavers',
    'Number of available beds per 1000 residents': 'Beds_per_1000',
    'Total nights spent in tourist accommodation establishments per resident population': 'Tourist_nights_per_resident',
    'Unemployment rate': 'Unemployment_rate',
    'Share of journeys to work by public transport (rail, metro, bus, tram) -%': 'Share_public_transport',
    'Share of journeys to work by car or motor cycle -%': 'Share_car_motorcycle',
    'Average time of journey to work - minutes': 'Avg_journey_minutes',
    'Number of registered cars per 1000 population': 'Cars_per_1000',
    'People killed in road accidents per 10000 pop.': 'Road_deaths_per_10000',
    'Total number of hours of sunshine per day': 'Sunshine_hours_per_day',
    'Average temperature of warmest month - degrees': 'Temp_warmest_month',
    'Average temperature of coldest month - degrees': 'Temp_coldest_month',
    'Rainfall - litre/mÂ˛': 'Rainfall_l_per_m2',
    'Annual average concentration of NO2 (Âµg/mÂł)': 'NO2_avg_conc',
    'Municipal waste generated (domestic and commercial), total - 1000 t': 'Municipal_waste_1000t'
}

df_wide = df_wide.rename(columns=rename_map)


# Lista wszystkich kolumn oprócz nazwy miasta
numeric_cols = df_wide.columns.drop('Geopolitical entity (declaring)')

# Konwersja na liczby
df_wide[numeric_cols] = df_wide[numeric_cols].apply(pd.to_numeric, errors='coerce')

# Teraz nazwy miast zostają, a reszta jest numeryczna
df_wide.info()


df_wide.describe().T


# usunięcie kolumn z dużą liczbą braków
df_wide = df_wide.loc[:, df_wide.count() >= 100]


# Poprawianie zmiennych
## po analizie irracjonalnych przypadków wychodzi na to, że prawdopodobnie wprowadzono wartości zgonów na 1 mln zamiast na 1 tys. urodzeń
df_wide['Infant_mortality_rate'] = df_wide['Infant_mortality_rate'].apply(lambda x: x/1000 if x >1000 else x)

# Deaths absolute → per 100k mieszkańców
if 'Deaths_under_65_circulatory_respiratory' in df_wide.columns and 'Population' in df_wide.columns:
    df_wide['Deaths_under_65_circ_resp_per_100k'] = (
        df_wide['Deaths_under_65_circulatory_respiratory'] / df_wide['Population'] * 100_000
    )


df_wide.describe().T





import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns



numeric_cols = df_wide.select_dtypes(include=['number']).columns.tolist()
X = df_wide[numeric_cols].fillna(0)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


inertia = []
K_range = range(2, 11)  # testujemy od 2 do 10 klastrów

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8,4))
plt.plot(K_range, inertia, marker='o')
plt.xlabel("Liczba klastrów (k)")
plt.ylabel("Inercja (suma kwadratów odległości od centroidów)")
plt.title("Metoda łokcia dla K-Means")
plt.show()



k = 6
kmeans = KMeans(n_clusters=k, random_state=42)
df_wide['Cluster'] = kmeans.fit_predict(X_scaled)



# Ile miast w każdym klastrze
print(df_wide['Cluster'].value_counts())

# Średnie wartości wskaźników w klastrach
cluster_summary = df_wide.groupby('Cluster')[numeric_cols].mean()
print(cluster_summary)

# Wizualizacja w dwóch wymiarach (PCA)
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
df_wide['PCA1'] = X_pca[:,0]
df_wide['PCA2'] = X_pca[:,1]

plt.figure(figsize=(12,10))
sns.scatterplot(data=df_wide, x='PCA1', y='PCA2', hue='Cluster', palette='tab10', s=100)
plt.title("Klastry miast – wizualizacja 2D PCA")
plt.show()



df_wide['Cluster'].value_counts()



cluster_summary = df_wide.groupby('Cluster')[numeric_cols].mean()
print(cluster_summary.T)  # transpozycja, żeby łatwiej czytać



# Pokazanie wszystkich miast i ich klastrów
df_wide[['Geopolitical entity (declaring)', 'Cluster']].sort_values('Cluster')



pd.set_option('display.max_rows', None)

# Wybieramy tylko unikalne wiersze z kodem miasta i nazwą
city_map = df_all[['Geopolitical entity (declaring)', 'cities']].drop_duplicates()

# Dołączamy do df_wide po kodzie miasta
df_wide = df_wide.merge(city_map,
                        on='Geopolitical entity (declaring)',
                        how='left')

# Sprawdzamy efekt
df_wide[['Geopolitical entity (declaring)', 'cities']].head(10)
df_wide[df_wide['Cluster'] == 1]


# centroidy (średnie w każdej zmiennej dla klastra)
centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X_scaled.columns)
centroids



# Różnica max-min między centroidami dla każdej zmiennej
importance = centroids.max() - centroids.min()
importance.sort_values(ascending=False)


centroids_orig = scaler.inverse_transform(centroids)
centroids_orig = pd.DataFrame(centroids_orig, columns=X_scaled.columns)
centroids_orig



plt.figure(figsize=(12,6))
sns.heatmap(centroids_orig, annot=True, cmap='coolwarm')
plt.xlabel('Variable')
plt.ylabel('Cluster')
plt.title('Centroids of each cluster (original scale)')
plt.show()




