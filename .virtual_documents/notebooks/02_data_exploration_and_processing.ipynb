





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import ipywidgets as widgets

from IPython.display import display
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer

pd.set_option('display.max_columns', None)


df = pd.read_csv('../data/preprocessed/preprocessed_data.csv')


df.describe().T





# Tworzymy przedziały populacji
bins = [0, 50_000, 100_000, 250_000, 500_000, 1_000_000, np.inf]
labels = ['0-50k', '50-100k', '100-250k', '250-500k', '500-1000k', '1000k+']

df['pop_group'] = pd.cut(df['Population'], bins=bins, labels=labels)

# Grupowanie i liczenie % braków bez kolumny grupującej
missing_matrix = (
    df.groupby('pop_group', observed=True)
      .apply(lambda x: x.drop(columns='pop_group', errors='ignore')  
                    .isna().mean() * 100)
)



result = df[df['pop_group'].isna() | (df['pop_group'] == '')][['City', 'Population', 'pop_group']]
result


plt.figure(figsize=(12,8))
sns.heatmap(missing_matrix.T, annot=True, fmt=".1f", cmap="Reds")  # .T = transpozycja
plt.title("% braków w kolumnach wg przedziałów populacji")
plt.xlabel("Przedział populacji")
plt.ylabel("Kolumny")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.show()





threshold = 0.5  # 50%
df = df.loc[:, df.isna().mean() <= threshold]


sns.set_style("white")

# wybór tylko kolumn numerycznych
num_cols = df.select_dtypes(include='number').columns

h_per_plot = 2.5

fig, axes = plt.subplots(
    nrows=len(num_cols),
    ncols=1,
    figsize=(10, h_per_plot * len(num_cols)),
    constrained_layout=True
)

# gdy jest tylko jedna kolumna
if len(num_cols) == 1:
    axes = [axes]

for ax, col in zip(axes, num_cols):
    ax.hist(
        df[col].dropna(),
        bins=30,
        color='#223A5E',
        alpha=1
    )
    ax.set_title(col, fontsize=10)
    ax.set_ylabel("Count")
    sns.despine(ax=ax)

axes[-1].set_xlabel("Value")

out = widgets.Output(
    layout=widgets.Layout(
        height='600px',
        overflow='auto',
        border='1px solid lightgray'
    )
)

with out:
    plt.show()

display(out)





# Zmienne odrzucone z analizy (brak sensownej imputacji)
cols_to_drop = [
    'Municipal_waste_1000t',
    'Deaths_under_65_circulatory_respiratory',
    'Beds_per_1000',
    'Tourist_nights_per_resident',
    'Waste_prod_rate',
    'pop_group'
]

# Usunięcie kolumn (bez błędu, jeśli którejś nie ma)
df = df.drop(columns=cols_to_drop, errors='ignore')


df.describe().T








df_before = df.copy()


# Lista zmiennych według metody imputacji
mean_vars = ["Infant_mortality_rate", "Crude_birth_rate", "Crude_death_rate"]
median_vars = ["Median_age"]
knn_vars = ["Share_foreigners", "Unemployment_rate"]
mice_log_vars = ["Cars_per_1000", "Population", "Share_students_higher_edu", "Road_deaths_per_10000"]
mice_vars = ["Age_dependency_ratio", "Old_dependency_ratio", "Young_dependency_ratio",
             "Avg_living_area_m2_per_person", "Deaths_under_65_circ_resp_per_100k"]


# Log-transformacja dla wybranych zmiennych
df_log = df.copy()
for col in mice_log_vars:
    df_log[col] = np.log1p(df_log[col])  # log1p obsłuży zera


# Imputacja średnią
if mean_vars:
    mean_imputer = SimpleImputer(strategy="mean")
    df[mean_vars] = mean_imputer.fit_transform(df[mean_vars])

# Imputacja medianą
if median_vars:
    median_imputer = SimpleImputer(strategy="median")
    df[median_vars] = median_imputer.fit_transform(df[median_vars])

# Imputacja KNN
if knn_vars:
    scaler_knn = StandardScaler()
    knn_scaled = scaler_knn.fit_transform(df[knn_vars])
    knn_imputer = KNNImputer(n_neighbors=5)
    knn_imputed = knn_imputer.fit_transform(knn_scaled)
    df[knn_vars] = scaler_knn.inverse_transform(knn_imputed)

# Imputacja MICE (IterativeImputer)
all_mice_vars = mice_vars + mice_log_vars
scaler_mice = StandardScaler()
mice_scaled = scaler_mice.fit_transform(df_log[all_mice_vars])
mice_imputer = IterativeImputer(random_state=42, max_iter=20)
mice_imputed = mice_imputer.fit_transform(mice_scaled)


# wracamy do oryginalnej skali
df_log[all_mice_vars] = scaler_mice.inverse_transform(mice_imputed)

# Cofnięcie log-transformacji
for col in mice_log_vars:
    df[col] = np.expm1(df_log[col])  # odwrotność log1p

# Kopiowanie zmiennych MICE do finalnego df
for col in mice_vars:
    df[col] = df_log[col]


# Sprawdzenie braków
print("Braki po imputacji:")
print(df.isna().sum())


df


# Wybór kolumn numerycznych (bez identyfikatorów)
num_cols = df.select_dtypes(include='number').columns

# Statystyki przed imputacją
stats_before = (
    df_before[num_cols]
    .agg(['mean', 'std', 'min', 'max'])
    .T
    .add_suffix('_before')
)

# Statystyki po imputacji
stats_after = (
    df[num_cols]
    .agg(['mean', 'std', 'min', 'max'])
    .T
    .add_suffix('_after')
)

# Połączenie w jedną tabelę
comparison_table = pd.concat([stats_before, stats_after], axis=1)

comparison_table['mean_change_%'] = (
    (comparison_table['mean_after'] - comparison_table['mean_before'])
    / comparison_table['mean_before'] * 100
)

comparison_table['std_change_%'] = (
    (comparison_table['std_after'] - comparison_table['std_before'])
    / comparison_table['std_before'] * 100
)

comparison_table








# Obliczamy liczność i sortujemy malejąco
counts = df['Country_code'].value_counts().sort_values(ascending=False)

# Tworzymy wykres poziomy
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=counts.values, y=counts.index, color='#223A5E')

# Dodajemy etykiety liczności na końcach słupków
for i, value in enumerate(counts.values):
    ax.text(value + 0.5, i, str(value), va='center') 

plt.title('Liczba rekordów dla każdego Country_code (malejąco)')
plt.xlabel('Liczba rekordów')
plt.ylabel('Country Code')
plt.show()





df_num = df.drop(['City', 'Country_code'], axis=1)
corr = df_num.corr(method='pearson')

plt.figure(figsize=(12, 10))
sns.heatmap(
    corr,
    cmap='coolwarm',
    center=0,
    vmin=-1,
    vmax=1,
    square=True,
    linewidths=0.5,
    annot=True,
    fmt=".2f",
    annot_kws={"size": 8}
)

plt.title("Macierz korelacji")
plt.tight_layout()
plt.show()





demog_corr = df[['Age_dependency_ratio', 'Crude_birth_rate',  'Crude_death_rate', 'Median_age', 'Old_dependency_ratio', 'Young_dependency_ratio']].corr()
demog_corr





df = df.drop(['Young_dependency_ratio', 'Crude_death_rate', 'Age_dependency_ratio', 'Median_age', 'Deaths_under_65_circ_resp_per_100k'], axis=1)


df.drop(['City', 'Country_code'], axis=1).corr()





num_vars = [
    'Avg_living_area_m2_per_person', 'Crude_birth_rate',
    'Share_foreigners', 'Infant_mortality_rate',
    'Cars_per_1000', 'Old_dependency_ratio', 'Road_deaths_per_10000',
    'Population', 'Share_students_higher_edu', 'Unemployment_rate'
]

scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[num_vars] = scaler.fit_transform(df[num_vars])


df.to_csv('../data/processed/processed_data_unscaled.csv', index=False)
df_scaled.to_csv('../data/processed/processed_data_scaled.csv', index=False)



